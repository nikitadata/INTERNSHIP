{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "  columns = row.find_all(\"td\")\n",
    "  name = columns[0].text.strip()\n",
    "  term = columns[1].text.strip()\n",
    "  \n",
    "  names.append(name)\n",
    "  terms.append(term)\n",
    "\n",
    "data = {\"Name\": names, \"Term of Office\": terms}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# displaying dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response_teams = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(response_teams.content, \"html.parser\")\n",
    "\n",
    "teams_data = []\n",
    "table_teams = soup_teams.find(\"table\", class_=\"table\")\n",
    "rows_teams = table_teams.find_all(\"tr\")\n",
    "\n",
    "for row in rows_teams[1:11]:\n",
    "  team_name = row.find(\"span\", class_=\"u-hide-phablet\").text.strip()\n",
    "  matches = row.find_all(\"td\")[2].text.strip()\n",
    "  points = row.find_all(\"td\")[3].text.strip()\n",
    "  rating = row.find_all(\"td\")[4].text.strip()\n",
    "  teams_data.append([team_name, matches, points, rating])\n",
    "\n",
    "url_batting = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response_batting = requests.get(url_batting)\n",
    "soup_batting = BeautifulSoup(response_batting.content, \"html.parser\")\n",
    "\n",
    "batting_data = []\n",
    "table_batting = soup_batting.find(\"table\", class_=\"table\")\n",
    "rows_batting = table_batting.find_all(\"tr\")\n",
    "\n",
    "for row in rows_batting[1:11]:\n",
    "  player_name = row.find(\"td\", class_=\"table-body__cell rankings-table__name name\").text.strip()\n",
    "  team = row.find(\"span\", class_=\"table-body__logo-text\").text.strip()\n",
    "  rating = row.find(\"td\", class_=\"table-body__cell rating\").text.strip()\n",
    "  batting_data.append([player_name, team, rating])\n",
    "\n",
    "url_allrounders = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response_allrounders = requests.get(url_allrounders)\n",
    "soup_allrounders = BeautifulSoup(response_allrounders.content, \"html.parser\")\n",
    "\n",
    "allrounders_data = []\n",
    "table_allrounders = soup_allrounders.find(\"table\", class_=\"table\")\n",
    "rows_allrounders = table_allrounders.find_all(\"tr\")\n",
    "\n",
    "for row in rows_allrounders[1:11]:\n",
    "  player_name = row.find(\"td\", class_=\"table-body__cell rankings-table__name name\").text.strip()\n",
    "  team = row.find(\"span\", class_=\"table-body__logo-text\").text.strip()\n",
    "  rating = row.find(\"td\", class_=\"table-body__cell rating\").text.strip()\n",
    "  allrounders_data.append([player_name, team, rating])\n",
    "\n",
    "# data frames\n",
    "df_teams = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "df_batting = pd.DataFrame(batting_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "df_allrounders = pd.DataFrame(allrounders_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "\n",
    "#data frames\n",
    "print(\"Top 10 ODI teams in women's cricket:\")\n",
    "print(df_teams)\n",
    "print(\"\\nTop 10 women's ODI Batting players:\")\n",
    "print(df_batting)\n",
    "print(\"\\nTop 10 women's ODI all-rounders:\")\n",
    "print(df_allrounders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "  headline = article.find(\"a\").text.strip()\n",
    "  headlines.append(headline)\n",
    "  \n",
    "  time = article.find(\"time\").text.strip()\n",
    "  times.append(time)\n",
    "  \n",
    "  link = article.find(\"a\")[\"href\"]\n",
    "  links.append(link)\n",
    "\n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "  # Scrape the title\n",
    "  title = article.find(\"h3\").text.strip()\n",
    "  titles.append(title)\n",
    "  \n",
    "  author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "  authors.append(author)\n",
    "  \n",
    "  date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "  dates.append(date)\n",
    "  \n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image['src'])\n",
    "\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
